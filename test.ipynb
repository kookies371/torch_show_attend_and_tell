{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'hello': 2, 'hi': 1})\n",
      "Counter({'bye': 1})\n",
      "Counter({'hello': 2, 'hi': 1, 'bye': -1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens = 'hello hello hi'.split(' ')\n",
    "tokens2 = 'bye'.split(' ')\n",
    "counter = Counter(tokens)\n",
    "counter2 = Counter(tokens2)\n",
    "\n",
    "print(counter)\n",
    "print(counter2)\n",
    "\n",
    "counter.subtract(counter2)\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2689, 0.7311],\n",
       "        [0.2689, 0.7311]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "\n",
    "a.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]])\n",
      "torch.Size([1, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "idx = torch.zeros([1, 2], dtype=torch.long)\n",
    "idx[0, 0] = 1\n",
    "idx[0, 1] = 2\n",
    "print(idx)\n",
    "\n",
    "a = F.one_hot(idx, num_classes = 10)\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 202654/420000 [00:01<00:01, 143006.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from torchtext.data import get_tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"../torch_datasets/annotations/captions_val2014.json\", \"r\") as f:\n",
    "    annotaion = json.load(f)\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "max_cap_len = 0\n",
    "for i in tqdm(range(420000)):\n",
    "    try:\n",
    "        caption = annotaion['annotations'][i]['caption']\n",
    "        tokens = tokenizer(caption)\n",
    "        cap_len = len(tokens)\n",
    "        max_cap_len = max_cap_len if max_cap_len > cap_len else cap_len\n",
    "    except:\n",
    "        break\n",
    "\n",
    "print(max_cap_len)\n",
    "# max_cap_len is 57!! (train 57, val 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82783/82783 [10:26<00:00, 132.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# prepare datasets\n",
    "train_dataset = datasets.CocoCaptions(\n",
    "    root=\"../torch_datasets/train2014\",\n",
    "    annFile=\"../torch_datasets/annotations/captions_train2014.json\",\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "val_dataset = datasets.CocoCaptions(\n",
    "    root=\"../torch_datasets/val2014\",\n",
    "    annFile=\"../torch_datasets/annotations/captions_val2014.json\",\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "max_cpi = 0\n",
    "for i, (img, captions) in enumerate(tqdm(train_dataset)):\n",
    "    cpi = len(captions)\n",
    "    max_cpi = cpi if cpi > max_cpi else max_cpi\n",
    "\n",
    "print(max_cpi) # max_cpi = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kookies371/miniconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset CocoCaptions\n",
      "    Number of datapoints: 82783\n",
      "    Root location: ../torch_datasets/train2014\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "hello\n",
      "mother fucker\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_dataset = datasets.CocoCaptions(\n",
    "    root=\"../torch_datasets/train2014\",\n",
    "    annFile=\"../torch_datasets/annotations/captions_train2014.json\",\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataset.test_attribute = 'hello'\n",
    "train_dataset.eka = \"mother fucker\"\n",
    "\n",
    "print(train_dataset)\n",
    "print(train_dataset.test_attribute)\n",
    "print(train_dataset.eka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "class AAA():\n",
    "    def __init__(self):\n",
    "        self.a = 'a'\n",
    "        self.b = 'b'\n",
    "\n",
    "aaa = AAA()\n",
    "print(aaa.a)\n",
    "print(aaa.b)\n",
    "\n",
    "aaa.c = 'c'\n",
    "print(aaa.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "['Closeup of bins of food that include broccoli and bread.', 'A meal is presented in brightly colored plastic trays.', 'there are containers filled with different kinds of foods', 'Colorful dishes holding meat, vegetables, fruit, and bread.', 'A bunch of trays that have different food.']\n",
      "['A giraffe eating food from the top of the tree.', 'A giraffe standing up nearby a tree ', 'A giraffe mother with its baby in the forest.', 'Two giraffes standing in a tree filled area.', 'A giraffe standing next to a forest filled with trees.']\n",
      "['A flower vase is sitting on a porch stand.', 'White vase with different colored flowers sitting inside of it. ', 'a white vase with many flowers on a stage', 'A white vase filled with different colored flowers.', 'A vase with red and white flowers outside on a sunny day.']\n",
      "['A zebra grazing on lush green grass in a field.', 'Zebra reaching its head down to ground where grass is. ', 'The zebra is eating grass in the sun.', 'A lone zebra grazing in some green grass.', 'a Zebra grazing on grass in a green open field.']\n"
     ]
    }
   ],
   "source": [
    "from dataset import CustomCOCOCaption\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "ds = CustomCOCOCaption(    \n",
    "    root=\"../torch_datasets/train2014\",\n",
    "    annFile=\"../torch_datasets/annotations/captions_train2014.json\",\n",
    "    transform=transforms.ToTensor(),\n",
    "    max_cpi = 100,\n",
    "    max_cap_len = 100,\n",
    ")\n",
    "\n",
    "for i, (img, captions) in enumerate(ds):\n",
    "    print(captions)\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from dataset import CustomCOCOCaption\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "def collate_fn(x):\n",
    "    # x = [(img, captions), ...]\n",
    "    imgs, captionss = [], []\n",
    "    for img, captions in x:\n",
    "        imgs.append(img)\n",
    "        captionss.append(captions)\n",
    "\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    return (imgs, captionss)\n",
    "\n",
    "\n",
    "train_dataset = CustomCOCOCaption(\n",
    "    root=\"../torch_datasets/train2014\",\n",
    "    annFile=\"../torch_datasets/annotations/captions_train2014.json\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((640, 640)),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    print(len(data[1][0]))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([3, 426, 640])\n",
      "torch.Size([3, 428, 640])\n",
      "torch.Size([3, 425, 640])\n",
      "torch.Size([3, 640, 481])\n",
      "torch.Size([3, 500, 381])\n",
      "torch.Size([3, 488, 640])\n",
      "torch.Size([3, 640, 480])\n",
      "torch.Size([3, 426, 640])\n",
      "torch.Size([3, 640, 427])\n",
      "torch.Size([3, 375, 500])\n",
      "torch.Size([3, 612, 612])\n",
      "torch.Size([3, 425, 640])\n",
      "torch.Size([3, 640, 512])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([3, 427, 640])\n",
      "torch.Size([3, 427, 640])\n",
      "torch.Size([3, 416, 640])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([3, 640, 416])\n",
      "torch.Size([3, 481, 640])\n",
      "torch.Size([3, 573, 640])\n",
      "torch.Size([3, 640, 480])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([3, 428, 640])\n",
      "torch.Size([3, 640, 480])\n",
      "torch.Size([3, 640, 427])\n",
      "torch.Size([3, 536, 640])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([3, 428, 640])\n",
      "torch.Size([3, 424, 640])\n"
     ]
    }
   ],
   "source": [
    "from dataset import CustomCOCOCaption\n",
    "\n",
    "train_dataset = CustomCOCOCaption(\n",
    "    root=\"../torch_datasets/train2014\",\n",
    "    annFile=\"../torch_datasets/annotations/captions_train2014.json\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "for i, (img, captions) in enumerate(train_dataset):\n",
    "    print(img.shape)\n",
    "\n",
    "    if i == 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list expected at most 1 argument, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bncia.snu.ac.kr/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bncia.snu.ac.kr/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000011vscode-remote?line=2'>3</a>\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bncia.snu.ac.kr/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000011vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39m*\u001b[39;49ma\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bncia.snu.ac.kr/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000011vscode-remote?line=4'>5</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bncia.snu.ac.kr/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000011vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(b)\n",
      "\u001b[0;31mTypeError\u001b[0m: list expected at most 1 argument, got 3"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "\n",
    "b = list(\n",
    "    *a\n",
    ")\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test push')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000],\n",
      "        [0.5331, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn((2, 2))\n",
    "b = a.relu()\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n",
      "tensor([[[[0.6863, 0.6431, 0.6235,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.6814, 0.6602, 0.6649,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.6842, 0.6634, 0.6758,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          ...,\n",
      "          [0.1334, 0.1457, 0.0935,  ..., 0.9361, 0.9577, 0.9537],\n",
      "          [0.1489, 0.1321, 0.0907,  ..., 0.9654, 0.9267, 0.9466],\n",
      "          [0.1294, 0.0980, 0.0980,  ..., 0.9922, 0.8902, 0.9490]],\n",
      "\n",
      "         [[0.5451, 0.5020, 0.4745,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.5402, 0.5190, 0.5159,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.5459, 0.5222, 0.5297,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.1605, 0.1796, 0.1298,  ..., 0.5899, 0.6086, 0.6047],\n",
      "          [0.1817, 0.1689, 0.1314,  ..., 0.6164, 0.5762, 0.5961],\n",
      "          [0.1647, 0.1373, 0.1412,  ..., 0.6431, 0.5373, 0.5961]],\n",
      "\n",
      "         [[0.4980, 0.4549, 0.4314,  ..., 0.9765, 0.9765, 0.9765],\n",
      "          [0.4932, 0.4719, 0.4727,  ..., 0.9765, 0.9765, 0.9765],\n",
      "          [0.4988, 0.4752, 0.4851,  ..., 0.9765, 0.9765, 0.9765],\n",
      "          ...,\n",
      "          [0.1698, 0.1800, 0.1302,  ..., 0.5650, 0.5801, 0.5733],\n",
      "          [0.1851, 0.1650, 0.1231,  ..., 0.5899, 0.5448, 0.5632],\n",
      "          [0.1608, 0.1333, 0.1255,  ..., 0.6118, 0.5059, 0.5608]]],\n",
      "\n",
      "\n",
      "        [[[0.6353, 0.5900, 0.7578,  ..., 0.4670, 0.5626, 0.6078],\n",
      "          [0.7020, 0.8248, 0.8870,  ..., 0.4981, 0.7469, 0.8471],\n",
      "          [0.8784, 0.9722, 0.9949,  ..., 0.6841, 0.6486, 0.7294],\n",
      "          ...,\n",
      "          [0.5804, 0.6968, 0.6911,  ..., 0.5463, 0.5123, 0.5608],\n",
      "          [0.6392, 0.7976, 0.7537,  ..., 0.5917, 0.4551, 0.5294],\n",
      "          [0.6235, 0.7399, 0.6649,  ..., 0.6156, 0.5580, 0.6000]],\n",
      "\n",
      "         [[0.6392, 0.5616, 0.7087,  ..., 0.2998, 0.3877, 0.5137],\n",
      "          [0.7098, 0.8132, 0.8630,  ..., 0.4406, 0.7752, 0.9529],\n",
      "          [0.8941, 0.9782, 0.9794,  ..., 0.7594, 0.7538, 0.8314],\n",
      "          ...,\n",
      "          [0.5569, 0.6829, 0.6765,  ..., 0.5083, 0.4791, 0.5373],\n",
      "          [0.5765, 0.7219, 0.7030,  ..., 0.5269, 0.4075, 0.4980],\n",
      "          [0.5608, 0.6836, 0.5934,  ..., 0.5809, 0.5358, 0.5843]],\n",
      "\n",
      "         [[0.7490, 0.6682, 0.8035,  ..., 0.2548, 0.3144, 0.3725],\n",
      "          [0.7686, 0.8785, 0.9325,  ..., 0.2725, 0.5530, 0.7373],\n",
      "          [0.9294, 0.9682, 0.9792,  ..., 0.5404, 0.5972, 0.6941],\n",
      "          ...,\n",
      "          [0.5569, 0.6668, 0.6458,  ..., 0.4970, 0.4629, 0.5373],\n",
      "          [0.5804, 0.7291, 0.7026,  ..., 0.5200, 0.3868, 0.4902],\n",
      "          [0.5608, 0.6804, 0.5950,  ..., 0.5348, 0.4779, 0.5490]]],\n",
      "\n",
      "\n",
      "        [[[0.1255, 0.1961, 0.2275,  ..., 0.6000, 0.5961, 0.6000],\n",
      "          [0.1196, 0.1589, 0.1864,  ..., 0.6117, 0.6039, 0.6000],\n",
      "          [0.1157, 0.1222, 0.1432,  ..., 0.6255, 0.6124, 0.5994],\n",
      "          ...,\n",
      "          [0.4470, 0.4685, 0.4651,  ..., 0.5760, 0.5793, 0.5954],\n",
      "          [0.4392, 0.4451, 0.4313,  ..., 0.5863, 0.5843, 0.5548],\n",
      "          [0.4353, 0.4314, 0.4118,  ..., 0.5843, 0.5765, 0.5176]],\n",
      "\n",
      "         [[0.0902, 0.1412, 0.1647,  ..., 0.4941, 0.4902, 0.4824],\n",
      "          [0.0765, 0.1079, 0.1393,  ..., 0.5000, 0.4922, 0.4824],\n",
      "          [0.0647, 0.0764, 0.1112,  ..., 0.5085, 0.4948, 0.4817],\n",
      "          ...,\n",
      "          [0.3797, 0.4077, 0.4116,  ..., 0.5021, 0.5139, 0.5300],\n",
      "          [0.3706, 0.3862, 0.3803,  ..., 0.5118, 0.5176, 0.4881],\n",
      "          [0.3647, 0.3725, 0.3608,  ..., 0.5098, 0.5098, 0.4510]],\n",
      "\n",
      "         [[0.0314, 0.0902, 0.1137,  ..., 0.3882, 0.3843, 0.3804],\n",
      "          [0.0294, 0.0550, 0.0727,  ..., 0.3921, 0.3882, 0.3804],\n",
      "          [0.0242, 0.0228, 0.0365,  ..., 0.3974, 0.3915, 0.3797],\n",
      "          ...,\n",
      "          [0.3503, 0.3783, 0.3789,  ..., 0.4766, 0.4845, 0.5006],\n",
      "          [0.3490, 0.3627, 0.3490,  ..., 0.4843, 0.4863, 0.4567],\n",
      "          [0.3490, 0.3529, 0.3294,  ..., 0.4824, 0.4784, 0.4196]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.4588, 0.4608, 0.4627,  ..., 0.0713, 0.0764, 0.0824],\n",
      "          [0.4588, 0.4608, 0.4627,  ..., 0.0784, 0.0823, 0.0863],\n",
      "          [0.4627, 0.4627, 0.4634,  ..., 0.0941, 0.0961, 0.0980],\n",
      "          ...,\n",
      "          [0.1412, 0.1431, 0.1444,  ..., 0.0124, 0.0079, 0.0039],\n",
      "          [0.1412, 0.1431, 0.1458,  ..., 0.0078, 0.0137, 0.0196],\n",
      "          [0.1412, 0.1451, 0.1483,  ..., 0.0032, 0.0137, 0.0235]],\n",
      "\n",
      "         [[0.4078, 0.4098, 0.4118,  ..., 0.0869, 0.0863, 0.0863],\n",
      "          [0.4078, 0.4098, 0.4118,  ..., 0.0824, 0.0862, 0.0902],\n",
      "          [0.4118, 0.4118, 0.4124,  ..., 0.0980, 0.1000, 0.1020],\n",
      "          ...,\n",
      "          [0.1451, 0.1471, 0.1483,  ..., 0.0242, 0.0196, 0.0157],\n",
      "          [0.1451, 0.1471, 0.1497,  ..., 0.0196, 0.0216, 0.0235],\n",
      "          [0.1451, 0.1490, 0.1523,  ..., 0.0150, 0.0215, 0.0275]],\n",
      "\n",
      "         [[0.3765, 0.3784, 0.3804,  ..., 0.0987, 0.1000, 0.1020],\n",
      "          [0.3765, 0.3784, 0.3804,  ..., 0.0980, 0.1019, 0.1059],\n",
      "          [0.3804, 0.3804, 0.3811,  ..., 0.1137, 0.1157, 0.1176],\n",
      "          ...,\n",
      "          [0.1608, 0.1628, 0.1640,  ..., 0.0438, 0.0392, 0.0353],\n",
      "          [0.1608, 0.1628, 0.1654,  ..., 0.0392, 0.0412, 0.0431],\n",
      "          [0.1608, 0.1647, 0.1680,  ..., 0.0346, 0.0411, 0.0471]]],\n",
      "\n",
      "\n",
      "        [[[0.3176, 0.2526, 0.2091,  ..., 0.7036, 0.7063, 0.6824],\n",
      "          [0.4118, 0.4597, 0.3243,  ..., 0.7987, 0.7637, 0.7569],\n",
      "          [0.2157, 0.4486, 0.5473,  ..., 0.7081, 0.7124, 0.7843],\n",
      "          ...,\n",
      "          [0.3098, 0.3817, 0.4045,  ..., 0.1868, 0.1397, 0.1569],\n",
      "          [0.3725, 0.4068, 0.4087,  ..., 0.1911, 0.1721, 0.1686],\n",
      "          [0.3922, 0.3887, 0.4285,  ..., 0.1880, 0.1696, 0.1490]],\n",
      "\n",
      "         [[0.5373, 0.4688, 0.4279,  ..., 0.7137, 0.7122, 0.7020],\n",
      "          [0.6157, 0.6362, 0.5062,  ..., 0.7550, 0.7338, 0.7373],\n",
      "          [0.4039, 0.6129, 0.7050,  ..., 0.6543, 0.6776, 0.7529],\n",
      "          ...,\n",
      "          [0.1922, 0.2675, 0.3063,  ..., 0.0949, 0.0417, 0.0588],\n",
      "          [0.2627, 0.3073, 0.3168,  ..., 0.0982, 0.0544, 0.0510],\n",
      "          [0.2980, 0.2912, 0.3304,  ..., 0.1040, 0.0794, 0.0588]],\n",
      "\n",
      "         [[0.2431, 0.1541, 0.0793,  ..., 0.4192, 0.4461, 0.4392],\n",
      "          [0.2941, 0.3249, 0.1933,  ..., 0.4722, 0.4784, 0.4784],\n",
      "          [0.0745, 0.3109, 0.4225,  ..., 0.3870, 0.4227, 0.4980],\n",
      "          ...,\n",
      "          [0.0588, 0.1342, 0.1606,  ..., 0.0334, 0.0142, 0.0314],\n",
      "          [0.1098, 0.1509, 0.1569,  ..., 0.0266, 0.0230, 0.0196],\n",
      "          [0.1255, 0.1289, 0.1696,  ..., 0.0241, 0.0103, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6627, 0.5059, 0.2627,  ..., 0.1294, 0.1255, 0.1255],\n",
      "          [0.6824, 0.5727, 0.3511,  ..., 0.1294, 0.1255, 0.1255],\n",
      "          [0.6980, 0.6432, 0.4517,  ..., 0.1294, 0.1255, 0.1255],\n",
      "          ...,\n",
      "          [0.3908, 0.3987, 0.4033,  ..., 0.0229, 0.0229, 0.0229],\n",
      "          [0.3941, 0.4020, 0.4078,  ..., 0.0314, 0.0314, 0.0314],\n",
      "          [0.3961, 0.4039, 0.4118,  ..., 0.0392, 0.0392, 0.0392]],\n",
      "\n",
      "         [[0.6706, 0.5255, 0.2824,  ..., 0.0314, 0.0275, 0.0275],\n",
      "          [0.6392, 0.5628, 0.3766,  ..., 0.0314, 0.0275, 0.0275],\n",
      "          [0.6184, 0.6112, 0.4765,  ..., 0.0314, 0.0275, 0.0275],\n",
      "          ...,\n",
      "          [0.4144, 0.4222, 0.4268,  ..., 0.0235, 0.0235, 0.0235],\n",
      "          [0.4176, 0.4255, 0.4314,  ..., 0.0314, 0.0314, 0.0314],\n",
      "          [0.4196, 0.4275, 0.4353,  ..., 0.0392, 0.0392, 0.0392]],\n",
      "\n",
      "         [[0.4863, 0.4078, 0.2039,  ..., 0.0157, 0.0118, 0.0118],\n",
      "          [0.5236, 0.4510, 0.2452,  ..., 0.0157, 0.0118, 0.0118],\n",
      "          [0.5542, 0.4954, 0.2909,  ..., 0.0157, 0.0118, 0.0118],\n",
      "          ...,\n",
      "          [0.4065, 0.4144, 0.4255,  ..., 0.0137, 0.0137, 0.0137],\n",
      "          [0.4098, 0.4216, 0.4314,  ..., 0.0274, 0.0274, 0.0274],\n",
      "          [0.4118, 0.4275, 0.4353,  ..., 0.0392, 0.0392, 0.0392]]]])\n",
      "tensor([[  0,  14, 754,  ...,   2,   2,   2],\n",
      "        [  0,  43, 124,  ...,   2,   2,   2],\n",
      "        [  0,  14, 988,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [  0,  14, 524,  ...,   2,   2,   2],\n",
      "        [  0,  14, 221,  ...,   2,   2,   2],\n",
      "        [  0,  14, 195,  ...,   2,   2,   2]])\n",
      "torch.Size([16, 102])\n"
     ]
    }
   ],
   "source": [
    "from dataset import CustomCOCOCaption\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = CustomCOCOCaption(\n",
    "    root='../torch_datasets/val2014',\n",
    "    annFile='../torch_datasets/annotations/captions_val2014.json',\n",
    "    transform=Compose([\n",
    "        ToTensor(),\n",
    "        Resize((640, 640))\n",
    "    ]),\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=train_dataset.collate_fn,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "for imgs, captionss in dataloader:\n",
    "    print(imgs)\n",
    "    print(captionss)\n",
    "\n",
    "    print(captionss.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "first argument must be callable or None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmsj/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000015vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmsj/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000015vscode-remote?line=2'>3</a>\u001b[0m test_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(a\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, b\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmsj/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000015vscode-remote?line=3'>4</a>\u001b[0m test_dd \u001b[39m=\u001b[39m defaultdict(test_dict)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmsj/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000015vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(test_dict)\n",
      "\u001b[0;31mTypeError\u001b[0m: first argument must be callable or None"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_dict = dict(a=1, b=2)\n",
    "test_dd = defaultdict(test_dict)\n",
    "print(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "squeeze() received an invalid combination of arguments - got (Tensor, dim=list), but expected one of:\n * (Tensor input)\n * (Tensor input, int dim)\n      didn't match because some of the arguments have invalid types: (Tensor, !dim=list!)\n * (Tensor input, name dim)\n      didn't match because some of the arguments have invalid types: (Tensor, !dim=list!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmsj/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmsj/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000016vscode-remote?line=2'>3</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmsj/home/kookies371/workspace/torch_show_attend_and_tell/test.ipynb#ch0000016vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49msqueeze(a, dim\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m])\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: squeeze() received an invalid combination of arguments - got (Tensor, dim=list), but expected one of:\n * (Tensor input)\n * (Tensor input, int dim)\n      didn't match because some of the arguments have invalid types: (Tensor, !dim=list!)\n * (Tensor input, name dim)\n      didn't match because some of the arguments have invalid types: (Tensor, !dim=list!)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.zeros((3, 1, 3, 1, 3))\n",
    "print(torch.squeeze(a, dim=[1, 3]).shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2e5a2fc120aea8fd8e3ca8a0be3a7cabdd6ecdbf637db27c66d81d888c5bd17"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
